{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2017/03/email-spam-filtering-an-implementation-with-python-and-scikit-learn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51 79]\n",
      " [58 72]]\n",
      "[[65 65]\n",
      " [64 66]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def make_Dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    \n",
    "    for item in list(dictionary):\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(5000)\n",
    "    return dictionary\n",
    "    \n",
    "def extract_features(mail_dir): \n",
    "    files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(files),5000))\n",
    "    docID = 0;\n",
    "    for fil in files:\n",
    "        with open(fil) as fi:\n",
    "            for i,line in enumerate(fi):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        wordID = 0\n",
    "                        for j,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordID = j\n",
    "                                features_matrix[docID,wordID] = words.count(word)\n",
    "            docID = docID + 1     \n",
    "    return features_matrix\n",
    "    \n",
    "# Create a dictionary of words with its frequency\n",
    "\n",
    "train_dir = 'train-mails'\n",
    "dictionary = make_Dictionary(train_dir)\n",
    "\n",
    "# Prepare feature vectors per training mail and its labels\n",
    "\n",
    "train_labels = np.zeros(702)\n",
    "train_labels[351:701] = 1\n",
    "train_matrix = extract_features(train_dir)\n",
    "\n",
    "# Training SVM and Naive bayes classifier and its variants\n",
    "\n",
    "model1 = LinearSVC()\n",
    "model2 = LogisticRegression()\n",
    "\n",
    "model1.fit(train_matrix,train_labels)\n",
    "model2.fit(train_matrix,train_labels)\n",
    "\n",
    "# Test the unseen mails for Spam\n",
    "\n",
    "test_dir = 'test-mails'\n",
    "test_matrix = extract_features(test_dir)\n",
    "test_labels = np.zeros(260)\n",
    "test_labels[130:260] = 1\n",
    "\n",
    "result1 = model1.predict(test_matrix)\n",
    "result2 = model2.predict(test_matrix)\n",
    "\n",
    "print (confusion_matrix(test_labels,result1))\n",
    "print (confusion_matrix(test_labels,result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> training result is bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import operator\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of letter : 702\n"
     ]
    }
   ],
   "source": [
    "# split word to ( , )\n",
    "\n",
    "train_dir = 'train-mails'\n",
    "emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "\n",
    "all_words = []\n",
    "for email in emails:\n",
    "    with open(email, encoding = 'ISO-8859-1') as m:\n",
    "        a = []\n",
    "        for i,line in enumerate(m):\n",
    "            if i == 2: #Bodt of email is only 2 line of text file\n",
    "                words_ = line\n",
    "                pattern3 = re.compile(\"[^\\w\\d]+\")\n",
    "                words_ = pattern3.sub(' ',words_)\n",
    "                words = words_.split()\n",
    "                for j in range(len(words)):\n",
    "                    a.append(words[j])\n",
    "    all_words.append(a)\n",
    "print ('size of letter :', len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dictionary : 5000\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_Dictionary(train_dir)\n",
    "print (\"size of dictionary :\", len(dictionary))\n",
    "np.save('dictionary.npy', dictionary)\n",
    "\n",
    "dictionary = np.load('dictionary.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "several submit overlook oversight fine example phenomenon english though mary retire perhap most submission date perhap current work various mean english preposition case difference various oversight overlook stem largely compound over over first seem fairly limit power analysis reveal number distinction mean quite indeed addition physical location over indicate power relationship both force authority value judgement general notion move another thing over skip over analysis oversight overlook one sense over mean authority over sense mean over perhap similar argument part compound instance own interest seem carry authority mean over analysis clear cut interest someone case half opinion own knowledge engineer center machine translation cmu want opinion carnegie mellon university pittsburgh pa usa pay \n"
     ]
    }
   ],
   "source": [
    "# word filter\n",
    "\n",
    "example_doc = []\n",
    "\n",
    "for letter in all_words:\n",
    "    example_line = ''\n",
    "    for w in letter:\n",
    "        if w in dictionary:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "print (example_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix : (702, 5259)\n"
     ]
    }
   ],
   "source": [
    "# converting data to vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 0.0, analyzer=\"word\")\n",
    "vectorizer.fit(example_doc)\n",
    "\n",
    "X_train =  vectorizer.transform(example_doc)\n",
    "print (\"sparse matrix :\", X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = LinearSVC()\n",
    "model2 = LogisticRegression()\n",
    "\n",
    "model1.fit(X_train,train_labels)\n",
    "model2.fit(X_train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of letter : 260\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "test_dir = 'test-mails'\n",
    "emails = [os.path.join(test_dir,f) for f in os.listdir(test_dir)]\n",
    "\n",
    "all_words = []\n",
    "for email in emails:\n",
    "    with open(email, encoding = 'ISO-8859-1') as m:\n",
    "        a = []\n",
    "        for i,line in enumerate(m):\n",
    "            if i == 2: #Bodt of email is only 2 line of text file\n",
    "                words_ = line\n",
    "                pattern3 = re.compile(\"[^\\w\\d]+\")\n",
    "                words_ = pattern3.sub(' ',words_)\n",
    "                words = words_.split()\n",
    "                for j in range(len(words)):\n",
    "                    a.append(words[j])\n",
    "    all_words.append(a)\n",
    "print ('size of letter :', len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacation most place earth florida special online promotional vacation package is limited don wait our website information price package info sorry offer available travel agent resale offer available world wide full detail http ma trip html must internet explorer \n"
     ]
    }
   ],
   "source": [
    "# word filter\n",
    "\n",
    "example_doc = []\n",
    "\n",
    "for letter in all_words:\n",
    "    example_line = ''\n",
    "    for w in letter:\n",
    "        if w in dictionary:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "print (example_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix : (260, 5259)\n"
     ]
    }
   ],
   "source": [
    "# converting data to vectors\n",
    "\n",
    "X_test =  vectorizer.transform(example_doc)\n",
    "print (\"sparse matrix :\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[70 60]\n",
      " [60 70]]\n",
      "[[66 64]\n",
      " [51 79]]\n"
     ]
    }
   ],
   "source": [
    "result1 = model1.predict(X_test)\n",
    "result2 = model2.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(test_labels,result1))\n",
    "print (confusion_matrix(test_labels,result2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> still bad..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enron-data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "N = 33716 #5172\n",
    "\n",
    "def make_Dictionary(root_dir):\n",
    "    emails_dirs = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]    \n",
    "    all_words = []       \n",
    "    for emails_dir in emails_dirs:\n",
    "        dirs = [os.path.join(emails_dir,f) for f in os.listdir(emails_dir)]\n",
    "        for d in dirs:\n",
    "            emails = [os.path.join(d,f) for f in os.listdir(d)]\n",
    "            for mail in emails:\n",
    "                with open(mail, encoding = 'ISO-8859-1') as m:\n",
    "                    for line in m:\n",
    "                        words = line.split()\n",
    "                        all_words += words\n",
    "    dictionary = Counter(all_words)\n",
    "    list_to_remove = dictionary.keys()\n",
    "    \n",
    "    for item in list(dictionary):\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary[item]\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "    \n",
    "    np.save('dict_enron.npy',dictionary) \n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "def extract_features(root_dir): \n",
    "    emails_dirs = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]  \n",
    "    docID = 0\n",
    "    features_matrix = np.zeros((N,3000))\n",
    "    train_labels = np.zeros(N)\n",
    "    for emails_dir in emails_dirs:\n",
    "        dirs = [os.path.join(emails_dir,f) for f in os.listdir(emails_dir)]\n",
    "        for d in dirs:\n",
    "            emails = [os.path.join(d,f) for f in os.listdir(d)]\n",
    "            for mail in emails:\n",
    "                with open(mail, encoding = 'ISO-8859-1') as m:\n",
    "                    all_words = []\n",
    "                    for line in m:\n",
    "                        words = line.split()\n",
    "                        all_words += words\n",
    "                    for word in all_words:\n",
    "                        wordID = 0\n",
    "                        for i,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                wordID = i\n",
    "                                features_matrix[docID,wordID] = all_words.count(word)\n",
    "                train_labels[docID] = int(mail.split(\".\")[-2] == 'spam')\n",
    "                docID = docID + 1                \n",
    "    return features_matrix, train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系統找不到指定的路徑。: 'Enron-data-set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ec86d8e5ca67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Enron-data-set'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_Dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9fd639b862b9>\u001b[0m in \u001b[0;36mmake_Dictionary\u001b[1;34m(root_dir)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_Dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0memails_dirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mall_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0memails_dir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0memails_dirs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系統找不到指定的路徑。: 'Enron-data-set'"
     ]
    }
   ],
   "source": [
    "#Create a dictionary of words with its frequency\n",
    "\n",
    "root_dir = 'Enron-data-set'\n",
    "dictionary = make_Dictionary(root_dir)\n",
    "\n",
    "\n",
    "#Prepare feature vectors per training mail and its labels\n",
    "\n",
    "features_matrix, labels = extract_features(root_dir)\n",
    "np.save('enron_features_matrix.npy',features_matrix)\n",
    "np.save('enron_labels.npy',labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33716, 3000)\n",
      "(33716,)\n",
      "16545 17171\n",
      "[[6448  154]\n",
      " [ 111 6774]]\n",
      "[[6394  208]\n",
      " [ 152 6733]]\n"
     ]
    }
   ],
   "source": [
    "#train_matrix = np.load('enron_features_matrix.npy');\n",
    "#labels = np.load('enron_labels.npy');\n",
    "print (features_matrix.shape)\n",
    "print (labels.shape)\n",
    "print (sum(labels==0),sum(labels==1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_matrix, labels, test_size=0.40)\n",
    "\n",
    "## Training models and its variants\n",
    "\n",
    "model1 = LinearSVC()\n",
    "model2 = MultinomialNB()\n",
    "\n",
    "model1.fit(X_train,y_train)\n",
    "model2.fit(X_train,y_train)\n",
    "\n",
    "result1 = model1.predict(X_test)\n",
    "result2 = model2.predict(X_test)\n",
    "\n",
    "print (confusion_matrix(y_test, result1))\n",
    "print (confusion_matrix(y_test, result2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'spam_model.sav'\n",
    "\n",
    "pickle.dump(model1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### real rowdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = 'spam_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features_realdata(root_dir):\n",
    "    emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]  \n",
    "    docID = 0\n",
    "    features_matrix = np.zeros((len(emails),3000))\n",
    "    for mail in emails:\n",
    "        print (mail)\n",
    "        with open(mail, encoding = 'ISO-8859-1') as m:\n",
    "            all_words = []\n",
    "            for line in m:\n",
    "                words = line.split()\n",
    "                all_words += words\n",
    "                for word in all_words:\n",
    "                    wordID = 0\n",
    "                    for i,d in enumerate(dictionary):\n",
    "                        if d[0] == word:\n",
    "                            wordID = i\n",
    "                            features_matrix[docID,wordID] = all_words.count(word)\n",
    "        docID = docID + 1                \n",
    "    return features_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/0014.1999-12-15.farmer.ham.txt\n",
      "test/Sample_000001_00000115.txt\n",
      "test/Sample_000001_00000039.txt\n",
      "test/0021.1999-12-15.farmer.ham.txt\n",
      "test/0011.1999-12-14.farmer.ham.txt\n",
      "test/0012.1999-12-14.farmer.ham.txt\n",
      "test/0010.1999-12-14.farmer.ham.txt\n",
      "test/Sample_000001_00000063.txt\n",
      "test/Sample_000001_00000089.txt\n",
      "test/0016.1999-12-15.farmer.ham.txt\n",
      "test/0015.1999-12-15.farmer.ham.txt\n",
      "test/0020.1999-12-15.farmer.ham.txt\n",
      "test/Sample_000001_00000019.txt\n",
      "test/Sample_000001_00000141.txt\n",
      "test/0013.1999-12-14.farmer.ham.txt\n",
      "test/0009.1999-12-14.farmer.ham.txt\n",
      "test/0019.1999-12-15.farmer.ham.txt\n",
      "test/0022.1999-12-16.farmer.ham.txt\n",
      "test/Sample_000001_00000211.txt\n",
      "test/Sample_000001_00000170.txt\n",
      "test/Sample_000001_00000186.txt\n",
      "test/Sample_000001_00000021.txt\n",
      "test/Sample_000001_00000154.txt\n",
      "test/Sample_000001_00000164.txt\n",
      "test/Sample_000001_00000074.txt\n",
      "test/Sample_000001_00000007.txt\n",
      "test/Sample_000001_00000030.txt\n",
      "test/Sample_000001_00000114.txt\n",
      "test/Sample_000001_00000150.txt\n",
      "(29, 3000)\n"
     ]
    }
   ],
   "source": [
    "real_dir = \"test\"\n",
    "real_matrix = extract_features_realdata(real_dir)\n",
    "print (real_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.predict(real_matrix)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
