{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_programs = np.load('cut_Programs.npy')\n",
    "cut_Question = np.load('cut_Questions.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Word Dictionary & Out-of-Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_word_dict(w):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for program in cut_programs:\n",
    "    for lines in program:\n",
    "        for line in lines:\n",
    "            for w in line:\n",
    "                add_word_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for question in cut_Question:\n",
    "    lines = question[0]\n",
    "    for line in lines:\n",
    "        for w in line:\n",
    "            add_word_dict(w)\n",
    "    \n",
    "    for i in range(1, 7):\n",
    "        line = question[i]\n",
    "        for w in line:\n",
    "            add_word_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('都', 92133), ('說', 91976), ('這個', 91211), ('到', 84288), ('他', 80208), ('也', 78973), ('去', 75817), ('什麼', 62431), ('喔', 61455), ('那', 60232)]\n"
     ]
    }
   ],
   "source": [
    "### important word\n",
    "VOC_SIZE = 30000\n",
    "VOC_START = 20\n",
    "\n",
    "voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "print(voc_dict[:10])\n",
    "np.save('voc_dict.npy', voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_dict = np.load('voc_dict.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Generating Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the format of question is to select one from six, our traing data only have continuous lines. Naively, i want to change the whole problem into a binary classification which means given two lines, my model want to judge these two are context or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "NUM_TRAIN = 100000\n",
    "TRAIN_VALID_RATE = 0.7\n",
    "NUM_PROGRAM = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    Xs, Ys = [], []\n",
    "    \n",
    "    for i in range(NUM_TRAIN):\n",
    "        pos_or_neg = random.randint(0, 1)\n",
    "        \n",
    "        if pos_or_neg==1:\n",
    "            program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "            line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "            \n",
    "            Xs.append([cut_programs[program_id][episode_id][line_id], \n",
    "                       cut_programs[program_id][episode_id][line_id+1]])\n",
    "            Ys.append(1)\n",
    "            \n",
    "        else:\n",
    "            first_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            first_episode_id = random.randint(0, len(cut_programs[first_program_id])-1)\n",
    "            first_line_id = random.randint(0, len(cut_programs[first_program_id][first_episode_id])-1)\n",
    "            \n",
    "            second_program_id = random.randint(0, NUM_PROGRAM-1)\n",
    "            second_episode_id = random.randint(0, len(cut_programs[second_program_id])-1)\n",
    "            second_line_id = random.randint(0, len(cut_programs[second_program_id][second_episode_id])-1)\n",
    "            \n",
    "            Xs.append([cut_programs[first_program_id][first_episode_id][first_line_id], \n",
    "                       cut_programs[second_program_id][second_episode_id][second_line_id]])\n",
    "            Ys.append(0)\n",
    "    \n",
    "    return Xs, Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train : 70000\n",
      "size of test : 30000\n"
     ]
    }
   ],
   "source": [
    "Xs, Ys = generate_training_data()\n",
    "\n",
    "x_train, y_train = Xs[:int(NUM_TRAIN*TRAIN_VALID_RATE)], Ys[:int(NUM_TRAIN*TRAIN_VALID_RATE)]\n",
    "x_valid, y_valid = Xs[int(NUM_TRAIN*TRAIN_VALID_RATE):], Ys[int(NUM_TRAIN*TRAIN_VALID_RATE):]\n",
    "\n",
    "print (\"size of train :\", len(x_train))\n",
    "print (\"size of test :\", len(x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 從cut_programs去生成資料，並建立模型。再將cut_Question變成兩句兩句為一個row，判斷0或1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence id : 0 31 647\n",
      "[['起'], ['一顆', '大西瓜']]\n",
      "Sentence id : 2 44 145\n",
      "[['其實', '又', '很', '違背', '你', '的', '人', '生觀'], ['就', '這種', '小氣', '的', '人']]\n",
      "Sentence id : 1 132 953\n",
      "[['那要', '熬', '就', '要', '熬', '的', '值', '得', '呀'], ['我要', '做', '一個', '完整', '的', '女人']]\n"
     ]
    }
   ],
   "source": [
    "### Example\n",
    "for i in range(3):\n",
    "    program_id = random.randint(0, NUM_PROGRAM-1) \n",
    "    episode_id = random.randint(0, len(cut_programs[program_id])-1)\n",
    "    line_id = random.randint(0, len(cut_programs[program_id][episode_id])-2)\n",
    "\n",
    "    print (\"Sentence id :\", program_id, episode_id, line_id)\n",
    "    print ([cut_programs[program_id][episode_id][line_id], cut_programs[program_id][episode_id][line_id+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['還好 天氣 不錯 ', '昨天 晚上 流星雨 ', '到 多 流星 ', '這次 收 穫 真 豐富 ', '當然 豐富 啦 ', '說 嘛 ', '精 心 製 作 宵 夜 ', '被 一個 人 吃 掉 一大半 ', '真 嗎 ', '不要 忘 記要 做 秘密 檔案 ']\n"
     ]
    }
   ],
   "source": [
    "example_doc = []\n",
    "\n",
    "for line in cut_programs[0][0]:\n",
    "    example_line = ''\n",
    "    for w in line:\n",
    "        if w in voc_dict:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "\n",
    "print( example_doc[:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: BoW (Bag-Of-Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabulary]\n",
      "\n",
      "還好 427\n",
      "天氣 172\n",
      "不錯 52\n",
      "昨天 255\n",
      "晚上 258\n",
      "流星雨 289\n",
      "流星 288\n",
      "這次 415\n",
      "豐富 392\n",
      "當然 317\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ngram_range=(min, max), default: 1-gram => (1, 1)\n",
    "count = CountVectorizer(ngram_range=(1, 1))\n",
    "\n",
    "count.fit(example_doc)\n",
    "BoW = count.vocabulary_\n",
    "print('[vocabulary]\\n')\n",
    "for key in list(BoW.keys())[:10]:\n",
    "    print('%s %d' % (key, BoW[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(did, vid)\ttf\n",
      "  (0, 52)\t1\n",
      "  (0, 172)\t1\n",
      "  (0, 427)\t1\n",
      "  (1, 255)\t1\n",
      "  (1, 258)\t1\n",
      "  (1, 289)\t1\n",
      "  (2, 288)\t1\n",
      "  (3, 392)\t1\n",
      "  (3, 415)\t1\n",
      "  (4, 317)\t1\n",
      "  (4, 392)\t1\n",
      "  (7, 9)\t1\n",
      "  (7, 11)\t1\n",
      "  (9, 49)\t1\n",
      "  (9, 273)\t1\n",
      "  (9, 331)\t1\n",
      "  (9, 376)\t1\n",
      "\n",
      "Is document-term matrix a scipy.sparse matrix? True\n"
     ]
    }
   ],
   "source": [
    "# get matrix (doc_id, vocabulary_id) --> tf\n",
    "doc_bag = count.transform(example_doc)\n",
    "print('(did, vid)\\ttf')\n",
    "print(doc_bag[:10])\n",
    "\n",
    "print('\\nIs document-term matrix a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "\n",
      "After calling .toarray(), is it a scipy.sparse matrix? False\n"
     ]
    }
   ],
   "source": [
    "doc_bag = doc_bag.toarray()\n",
    "print(doc_bag[:10])\n",
    "\n",
    "print('\\nAfter calling .toarray(), is it a scipy.sparse matrix? {}'.format(sp.sparse.issparse(doc_bag)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "蟋蟀: 99\n",
      "聲音: 19\n",
      "這樣: 19\n",
      "這個: 16\n",
      "還有: 16\n",
      "可以: 15\n",
      "所以: 14\n",
      "什麼: 14\n",
      "探索: 13\n",
      "時候: 12\n"
     ]
    }
   ],
   "source": [
    "doc_bag = count.fit_transform(example_doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 10\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]))[0][bag_cnts.argsort()[::-1][:top]], \n",
    "                  np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('%s: %d' % (tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 473)\n"
     ]
    }
   ],
   "source": [
    "print (doc_bag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: TF-IDF (Term-Frequency & Inverse-Document-Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "蟋蟀: 2.86\n",
      "這樣: 4.46\n",
      "聲音: 4.51\n",
      "這個: 4.62\n",
      "還有: 4.62\n",
      "可以: 4.68\n",
      "所以: 4.75\n",
      "什麼: 4.75\n",
      "探索: 4.82\n",
      "時候: 4.89\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "蟋蟀: 44.998239\n",
      "這樣: 13.750685\n",
      "這個: 10.831737\n",
      "聲音: 10.469564\n",
      "還有: 10.007670\n",
      "可以: 9.158488\n",
      "豆油伯: 8.888211\n",
      "什麼: 8.864458\n",
      "知道: 8.284927\n",
      "時候: 7.708157\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "tfidf.fit(example_doc)\n",
    "\n",
    "top = 10\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' % (tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(example_doc).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]))[0][tfidf_sum.argsort()[::-1]][:top], \n",
    "                  np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('%s: %f' % (tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 473)\n"
     ]
    }
   ],
   "source": [
    "print(doc_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec: Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**6)\n",
    "\n",
    "doc_hash = hashvec.transform(example_doc)\n",
    "print(doc_hash.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['所以', '我', '覺得', '他', '的', '成績', '這麼', '好'], ['或', '是', '曾經', '看過']] 0\n",
      "[['可是', '我', '還是', '不', '懂', '說'], ['為', '什麼', '要', '全部', '把', '它', '拆開']] 1\n",
      "[['非常', '盡心', '盡力', '的', '在', '照', '顧亞亞'], ['老師', ',', ' ', '那現', '在', '我們', '燈架', '好', '啦']] 0\n",
      "[['我', '嗎'], ['我覺', '得', '你', '們', '做', '出版社']] 0\n",
      "[['我', '跟', '平凡', '人', '是', '一模', '一樣', '的'], ['然', '後', '這裡', '可以', '看', '到']] 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print (x_valid[i], y_valid[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_doc = []\n",
    "\n",
    "for line in x_valid:\n",
    "    example_line = ''\n",
    "    for i in [0, 1]:\n",
    "        for w in line[i]:\n",
    "            if w in voc_dict:\n",
    "                example_line += w+' '\n",
    "        \n",
    "    valid_doc.append(example_line)\n",
    "\n",
    "print( valid_doc[:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_doc = []\n",
    "\n",
    "for line in x_train:\n",
    "    example_line = ''\n",
    "    for i in [0, 1]:\n",
    "        for w in line[i]:\n",
    "            if w in voc_dict:\n",
    "                example_line += w+' '\n",
    "        \n",
    "    train_doc.append(example_line)\n",
    "\n",
    "print( train_doc[:10] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashvec = HashingVectorizer(n_features=2**20)\n",
    "\n",
    "doc_hash_train = hashvec.transform(train_doc)\n",
    "doc_hash_val = hashvec.transform(valid_doc)\n",
    "\n",
    "print (doc_hash_train.shape, doc_hash_val.shape)\n",
    "print (len(y_train), sum(y_train), len(y_valid), sum(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\n",
    "\n",
    "lr = LogisticRegression(C=1000)\n",
    "sgd = SGDClassifier(loss='log', n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 1458\n",
      "Accuracy: 0.51\n",
      "roc_auc_score: 0.504827042291\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "lr.fit(doc_hash_train, y_train)\n",
    "\n",
    "# testing\n",
    "y_pred = lr.predict(doc_hash_val)\n",
    "score = roc_auc_score(y_valid, lr.predict_proba(doc_hash_val)[:,1])\n",
    "\n",
    "print ('Misclassified samples: %d' % (y_valid != y_pred).sum())\n",
    "print ('Accuracy: %.2f' % accuracy_score(y_valid, y_pred))\n",
    "print ('roc_auc_score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 1487\n",
      "Accuracy: 0.50\n",
      "roc_auc_score: 0.506629926906\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "sgd.fit(doc_hash_train, y_train)\n",
    "\n",
    "# testing\n",
    "y_pred = sgd.predict(doc_hash_val)\n",
    "score = roc_auc_score(y_valid, sgd.predict_proba(doc_hash_val)[:,1])\n",
    "\n",
    "print ('Misclassified samples: %d' % (y_valid != y_pred).sum())\n",
    "print ('Accuracy: %.2f' % accuracy_score(y_valid, y_pred))\n",
    "print ('roc_auc_score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
