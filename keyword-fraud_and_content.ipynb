{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import operator\n",
    "import collections\n",
    "import numpy as np\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of letter : 21083\n"
     ]
    }
   ],
   "source": [
    "# split word to ( , )\n",
    "\n",
    "train_dir = 'trend_data1'\n",
    "emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "\n",
    "all_words = []\n",
    "for email in emails:\n",
    "    with open(email, encoding = 'ISO-8859-1') as m:\n",
    "        a = []\n",
    "        for i,line in enumerate(m):\n",
    "            if i != 1:\n",
    "                words_ = line\n",
    "                pattern3 = re.compile(\"[^\\w\\d]+\")\n",
    "                words_ = pattern3.sub(' ',words_)\n",
    "                words = words_.split()\n",
    "                for j in range(len(words)):\n",
    "                    a.append(words[j])\n",
    "    all_words.append(a)\n",
    "print ('size of letter :', len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('all_words.npy', all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_programs = np.load('all_words.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate dictionary\n",
    "\n",
    "word_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_word_dict(w):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for letter in cut_programs:\n",
    "    for w in letter:\n",
    "        add_word_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word : 71742\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print ('size of word :', len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('com', 3725), ('all', 3515), ('DIV', 3478), ('s', 3465), ('The', 3423), ('OF', 3407), ('20', 3377), ('Ã¤', 3192), ('any', 3136), ('an', 2965)]\n"
     ]
    }
   ],
   "source": [
    "# choose word by frequence 10 < f\n",
    "\n",
    "VOC_SIZE = 5000\n",
    "VOC_START = 50\n",
    "\n",
    "voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "print(voc_dict[:10])\n",
    "np.save('keyword_voc_dict.npy', voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_dict = np.load('keyword_voc_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name Sani Abacha 2Cthe former first also wife former late head state Federal Republic Nigeria Gen Abacha whose sudden death came 8th June Since death 2CI been into present civilian administration been physical torture security agents country My son Mohammed under before federal high court Nigeria an he did commit he has been released since based certain conditions he allowed move around any where has been own town only also been placed under government watch As widow so lost confidence anybody within country You must heard over media reports internet various huge sums deposited husband different security firms abroad some companies give up their secrets disclosed lodged there many out right In fact total sum discovered Government so far tune 700 dollars And they make poor life 2EI got contacts through personal research 2Cand out decided reach through medium 2EI give more information regard soon reply repose great confidence hence due security network placed day day affairs cannot afford visit embassy so why decided contact hope betray confidence deposited sum 40 dollars security firm abroad whose name now until open communication shall grateful if could receive fund into safe keeping This arrangement known son alone son deal directly security up whole being 2EI seriously considering settle down abroad friendly like yours soon fund get into so start all over again if only wish if impossible help keep funds which 25 fund Please honesty watch word transaction 2EI require telephone fax numbers so commence communication immediately give more detailed picture things case dont accept please do let out security giving information total trust confidence greatly appreciate if accept proposal good faith Please action Sincerely yours Abacha \n"
     ]
    }
   ],
   "source": [
    "# word filter\n",
    "\n",
    "example_doc = []\n",
    "\n",
    "for letter in cut_programs:\n",
    "    example_line = ''\n",
    "    for w in letter:\n",
    "        if w in voc_dict:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "\n",
    "print( example_doc[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nems\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-29 06:38:22,129 : INFO : collecting all words and their counts\n",
      "2017-10-29 06:38:22,131 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2017-10-29 06:38:22,133 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-29 06:38:22,577 : INFO : PROGRESS: at sentence #10000, processed 4525456 words, keeping 85 word types\n",
      "2017-10-29 06:38:22,633 : INFO : PROGRESS: at sentence #20000, processed 5026202 words, keeping 86 word types\n",
      "2017-10-29 06:38:22,636 : INFO : collected 86 word types from a corpus of 5041719 raw words and 21083 sentences\n",
      "2017-10-29 06:38:22,638 : INFO : Loading a fresh vocabulary\n",
      "2017-10-29 06:38:22,640 : INFO : min_count=1 retains 86 unique words (100% of original 86, drops 0)\n",
      "2017-10-29 06:38:22,642 : INFO : min_count=1 leaves 5041719 word corpus (100% of original 5041719, drops 0)\n",
      "2017-10-29 06:38:22,644 : INFO : deleting the raw counts dictionary of 86 items\n",
      "2017-10-29 06:38:22,646 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2017-10-29 06:38:22,648 : INFO : downsampling leaves estimated 1295545 word corpus (25.7% of prior 5041719)\n",
      "2017-10-29 06:38:22,649 : INFO : estimated required memory for 86 words and 200 dimensions: 197800 bytes\n",
      "2017-10-29 06:38:22,651 : INFO : constructing a huffman tree from 86 words\n",
      "2017-10-29 06:38:22,656 : INFO : built huffman tree with maximum node depth 15\n",
      "2017-10-29 06:38:22,658 : INFO : resetting layer weights\n",
      "2017-10-29 06:38:22,662 : INFO : training model with 3 workers on 86 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 negative=0 window=5\n",
      "2017-10-29 06:38:23,669 : INFO : PROGRESS: at 2.42% examples, 878190 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-29 06:38:24,669 : INFO : PROGRESS: at 21.04% examples, 856484 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-29 06:38:25,669 : INFO : PROGRESS: at 40.13% examples, 860694 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-29 06:38:26,669 : INFO : PROGRESS: at 44.00% examples, 865556 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-29 06:38:27,671 : INFO : PROGRESS: at 60.99% examples, 843232 words/s, in_qsize 5, out_qsize 2\n",
      "2017-10-29 06:38:28,672 : INFO : PROGRESS: at 80.04% examples, 845934 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-29 06:38:29,675 : INFO : PROGRESS: at 82.81% examples, 849183 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-29 06:38:30,101 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-29 06:38:30,103 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-29 06:38:30,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-29 06:38:30,108 : INFO : training on 25208595 raw words (6322169 effective words) took 7.4s, 849507 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(example_doc, min_count=1, size=200, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-29 06:50:45,965 : INFO : saving Word2Vec object under word2vec.sav, separately None\n",
      "2017-10-29 06:50:45,967 : INFO : not storing attribute syn0norm\n",
      "2017-10-29 06:50:45,970 : INFO : not storing attribute cum_table\n",
      "2017-10-29 06:50:45,989 : INFO : saved word2vec.sav\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bloblist = []\n",
    "for i in range(len(example_doc)):\n",
    "    blob = tb(example_doc[i])\n",
    "    bloblist.append(blob)\n",
    "print (len(bloblist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_n = 10\n",
    "\n",
    "key_list = []\n",
    "for i, blob in enumerate(bloblist):\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    tmp = \"\"\n",
    "    for word, score in sorted_words[ :kw_n]:\n",
    "        tmp += word + ' '\n",
    "    key_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letter_score = []\n",
    "for letter in key_list:\n",
    "    if len(letter) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        score = model.score(letter.split())\n",
    "        letter_score.append(score.reshape(1, score.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> how to get keyword of fraud ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "result = 1 - spatial.distance.cosine(letter_score[0], letter_score[1])\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
