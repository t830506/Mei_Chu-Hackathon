{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import operator\n",
    "import collections\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of letter : 8915\n"
     ]
    }
   ],
   "source": [
    "# split word to ( , )\n",
    "\n",
    "train_dir = 'Mail_Plain_Content'\n",
    "emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "\n",
    "all_words = []\n",
    "for email in emails:\n",
    "    with open(email, encoding = 'ISO-8859-1') as m:\n",
    "        a = []\n",
    "        for i,line in enumerate(m):\n",
    "            if i != 1:\n",
    "                words_ = line\n",
    "                pattern3 = re.compile(\"[^\\w\\d]+\")\n",
    "                words_ = pattern3.sub(' ',words_)\n",
    "                words = words_.split()\n",
    "                for j in range(len(words)):\n",
    "                    a.append(words[j])\n",
    "    all_words.append(a)\n",
    "print ('size of letter :', len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('all_words.npy', all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cut_programs = np.load('all_words.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate dictionary\n",
    "\n",
    "word_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_word_dict(w):\n",
    "    if not w in word_dict:\n",
    "        word_dict[w] = 1\n",
    "    else:\n",
    "        word_dict[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for letter in cut_programs:\n",
    "    for w in letter:\n",
    "        add_word_dict(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of word : 106281\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "word_dict = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print ('size of word :', len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ID', 526), ('code', 526), ('Aug', 525), ('²æ', 524), ('support', 523), ('United', 522), ('Free', 522), ('offers', 520), ('loss', 518), ('next', 515)]\n"
     ]
    }
   ],
   "source": [
    "# choose word by frequence 10 < f\n",
    "\n",
    "VOC_SIZE = 15000\n",
    "VOC_START = 650\n",
    "\n",
    "voc_dict = word_dict[VOC_START:VOC_START+VOC_SIZE]\n",
    "print(voc_dict[:10])\n",
    "np.save('voc_dict.npy', voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voc_dict = np.load('voc_dict.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris working outside UK Fire Industry lifestyle decision Chris part leaves wishes Senior Management Team resend sales One Sales Team within 24 hours require immediate assistance Office Manager Paul Technical Support able fully assist Many thanks humble temporary inconvenience Yours sincerely Chris UK Sales Distribution Manager \n"
     ]
    }
   ],
   "source": [
    "# word filter\n",
    "\n",
    "example_doc = []\n",
    "\n",
    "for letter in cut_programs:\n",
    "    example_line = ''\n",
    "    for w in letter:\n",
    "        if w in voc_dict:\n",
    "            example_line += w+' '\n",
    "        \n",
    "    example_doc.append(example_line)\n",
    "\n",
    "print( example_doc[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "import gensim, logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-28 20:03:38,315 : INFO : collecting all words and their counts\n",
      "2017-10-28 20:03:38,317 : WARNING : Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "2017-10-28 20:03:38,318 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-10-28 20:03:38,670 : INFO : collected 133 word types from a corpus of 6014253 raw words and 8915 sentences\n",
      "2017-10-28 20:03:38,671 : INFO : Loading a fresh vocabulary\n",
      "2017-10-28 20:03:38,671 : INFO : min_count=1 retains 133 unique words (100% of original 133, drops 0)\n",
      "2017-10-28 20:03:38,672 : INFO : min_count=1 leaves 6014253 word corpus (100% of original 6014253, drops 0)\n",
      "2017-10-28 20:03:38,672 : INFO : deleting the raw counts dictionary of 133 items\n",
      "2017-10-28 20:03:38,673 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2017-10-28 20:03:38,674 : INFO : downsampling leaves estimated 1695942 word corpus (28.2% of prior 6014253)\n",
      "2017-10-28 20:03:38,674 : INFO : estimated required memory for 133 words and 200 dimensions: 305900 bytes\n",
      "2017-10-28 20:03:38,675 : INFO : constructing a huffman tree from 133 words\n",
      "2017-10-28 20:03:38,677 : INFO : built huffman tree with maximum node depth 17\n",
      "2017-10-28 20:03:38,678 : INFO : resetting layer weights\n",
      "2017-10-28 20:03:38,681 : INFO : training model with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 negative=0 window=5\n",
      "2017-10-28 20:03:39,682 : INFO : PROGRESS: at 15.17% examples, 1244168 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-28 20:03:40,685 : INFO : PROGRESS: at 30.02% examples, 1252642 words/s, in_qsize 6, out_qsize 0\n",
      "2017-10-28 20:03:41,688 : INFO : PROGRESS: at 45.16% examples, 1257267 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-28 20:03:42,689 : INFO : PROGRESS: at 58.98% examples, 1236865 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-28 20:03:43,689 : INFO : PROGRESS: at 74.58% examples, 1243596 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-28 20:03:44,693 : INFO : PROGRESS: at 89.53% examples, 1247306 words/s, in_qsize 5, out_qsize 0\n",
      "2017-10-28 20:03:45,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-10-28 20:03:45,387 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-10-28 20:03:45,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-10-28 20:03:45,389 : INFO : training on 30071265 raw words (8386540 effective words) took 6.7s, 1250355 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(example_doc, min_count=1, size=200, hs=1, negative=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nems/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8915\n"
     ]
    }
   ],
   "source": [
    "bloblist = []\n",
    "for i in range(len(example_doc)):\n",
    "    blob = tb(example_doc[i])\n",
    "    bloblist.append(blob)\n",
    "print (len(bloblist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kw_n = 10\n",
    "\n",
    "key_list = []\n",
    "for i, blob in enumerate(bloblist[:10]):\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    tmp = \"\"\n",
    "    for word, score in sorted_words[ :kw_n]:\n",
    "        tmp += word + ' '\n",
    "    key_list.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-28 20:41:26,634 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,637 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,637 : INFO : scoring 10 sentences took 0.0s, 3881 sentences/s\n",
      "2017-10-28 20:41:26,638 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,639 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,640 : INFO : scoring 10 sentences took 0.0s, 6809 sentences/s\n",
      "2017-10-28 20:41:26,640 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,642 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,643 : INFO : scoring 10 sentences took 0.0s, 4842 sentences/s\n",
      "2017-10-28 20:41:26,643 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,645 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,646 : INFO : scoring 10 sentences took 0.0s, 4982 sentences/s\n",
      "2017-10-28 20:41:26,646 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,648 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,649 : INFO : scoring 8 sentences took 0.0s, 3932 sentences/s\n",
      "2017-10-28 20:41:26,649 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,651 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,652 : INFO : scoring 10 sentences took 0.0s, 4284 sentences/s\n",
      "2017-10-28 20:41:26,653 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,654 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,655 : INFO : scoring 10 sentences took 0.0s, 5923 sentences/s\n",
      "2017-10-28 20:41:26,655 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,656 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,657 : INFO : scoring 10 sentences took 0.0s, 8460 sentences/s\n",
      "2017-10-28 20:41:26,658 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,659 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,660 : INFO : scoring 10 sentences took 0.0s, 7173 sentences/s\n",
      "2017-10-28 20:41:26,660 : INFO : scoring sentences with 3 workers on 133 vocabulary and 200 features, using sg=0 hs=1 sample=0.001 and negative=0\n",
      "2017-10-28 20:41:26,661 : INFO : reached end of input; waiting to finish 1 outstanding jobs\n",
      "2017-10-28 20:41:26,661 : INFO : scoring 9 sentences took 0.0s, 9424 sentences/s\n"
     ]
    }
   ],
   "source": [
    "letter_score = []\n",
    "for letter in key_list:\n",
    "    if len(letter) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        score = model.score(letter.split())\n",
    "        letter_score.append(score.reshape(1, score.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778173508768\n"
     ]
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "result = 1 - spatial.distance.cosine(letter_score[0], letter_score[1])\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-17.50776291, -19.54599953, -14.43978691, -23.67828941,\n",
       "          -6.4330616 , -16.28676033, -21.12831688, -40.6035881 ,\n",
       "         -17.72745132, -39.48973465]], dtype=float32),\n",
       " array([[-20.16458511, -26.20037651, -22.28595161, -21.14113045,\n",
       "         -15.49653721, -18.25351715, -15.35197639,  -7.70956469,\n",
       "         -24.88040924,  -9.64005661]], dtype=float32),\n",
       " array([[-27.44132614, -23.98354149, -26.61302757, -18.2983284 ,\n",
       "         -18.84646606, -21.57950974, -27.79431725, -13.44624901,\n",
       "         -16.00395012, -20.61208725]], dtype=float32),\n",
       " array([[-23.45058632, -14.10188961, -18.60039711, -21.87195396,\n",
       "         -19.3116436 , -27.94157028, -21.9922924 ,  -8.9779377 ,\n",
       "         -19.46103859, -15.48441792]], dtype=float32),\n",
       " array([[-11.28796673, -14.58249092, -15.78649426, -17.14369965,\n",
       "          -3.86019588,  -7.18644762, -12.38673973, -10.57828903]], dtype=float32),\n",
       " array([[-53.72369003, -21.31909943, -12.45995522, -30.64398003,\n",
       "         -18.61462784, -32.48761368, -17.97270203, -22.3100872 ,\n",
       "         -16.23841667, -31.86107635]], dtype=float32),\n",
       " array([[-30.98640633,  -8.53627968,  -8.35734081, -19.8338089 ,\n",
       "          -8.43328571, -30.44742775, -17.02812576, -18.58064461,\n",
       "         -23.31413841, -11.8904314 ]], dtype=float32),\n",
       " array([[-63.53989029, -14.24416256, -12.70391846, -15.18775368,\n",
       "         -15.37032795, -12.11604691, -24.25361633, -24.79464912,\n",
       "         -26.55989075, -20.54772949]], dtype=float32),\n",
       " array([[-44.75642776, -24.17084694, -19.99220276, -11.40750408,\n",
       "         -17.49931145, -14.39087963, -13.64879894, -19.5666008 ,\n",
       "         -37.20965195, -10.09689331]], dtype=float32),\n",
       " array([[-21.78187561, -23.1080761 , -10.3041935 , -18.39250946,\n",
       "         -14.99518871,  -8.7094574 , -24.64097023,  -2.41781473,\n",
       "          -6.34697342]], dtype=float32)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
